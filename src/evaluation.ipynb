{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A matching Triton is not available, some optimizations will not be enabled\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Anaconda\\envs\\control\\lib\\site-packages\\xformers\\__init__.py\", line 55, in _is_triton_available\n",
      "    from xformers.triton.softmax import softmax as triton_softmax  # noqa\n",
      "  File \"d:\\Anaconda\\envs\\control\\lib\\site-packages\\xformers\\triton\\softmax.py\", line 11, in <module>\n",
      "    import triton\n",
      "ModuleNotFoundError: No module named 'triton'\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path \n",
    "import os\n",
    "\n",
    "project_main_path = Path.cwd().parent\n",
    "assert project_main_path.name == 'EC523_Project_G'\n",
    "added_path = os.path.abspath(project_main_path.__str__())\n",
    "if added_path not in os.sys.path:\n",
    "    os.sys.path.append(added_path)  \n",
    "\n",
    "import sys\n",
    "import diffusers\n",
    "import importlib\n",
    "importlib.reload(diffusers)\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "from IPython.utils import io\n",
    "import importlib \n",
    "from lib import data as anime_data\n",
    "import lib.controlnet_self as controlnet_self_file\n",
    "from lib.controlnet_self import ControlNetModel_SELF as ControlNetModel\n",
    "from lib.controlnet_self import MultiControlNetModel_SELF \n",
    "importlib.reload(anime_data)\n",
    "importlib.reload(controlnet_self_file)\n",
    "import argparse\n",
    "import contextlib\n",
    "import gc\n",
    "import logging\n",
    "import math\n",
    "import random\n",
    "import shutil\n",
    "import accelerate\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.checkpoint\n",
    "import transformers\n",
    "from accelerate import Accelerator\n",
    "from accelerate.logging import get_logger\n",
    "from accelerate.utils import ProjectConfiguration, set_seed\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import create_repo, upload_folder\n",
    "from packaging import version\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoTokenizer, PretrainedConfig\n",
    "import diffusers\n",
    "from lib.controlnet_self import ControlNetModel_SELF as ControlNetModel\n",
    "from lib.controlnet_self import MultiControlNetModel_SELF\n",
    "from diffusers import (\n",
    "    AutoencoderKL,\n",
    "    # ControlNetModel,\n",
    "    DDPMScheduler,\n",
    "    StableDiffusionControlNetPipeline,\n",
    "    UNet2DConditionModel,\n",
    "    UniPCMultistepScheduler,\n",
    ")\n",
    "from diffusers.optimization import get_scheduler\n",
    "from diffusers.utils import check_min_version, is_wandb_available\n",
    "from diffusers.utils.hub_utils import load_or_create_model_card, populate_model_card\n",
    "from diffusers.utils.import_utils import is_xformers_available\n",
    "from diffusers.utils.torch_utils import is_compiled_module\n",
    "import wandb\n",
    "importlib.reload(anime_data)\n",
    "from huggingface_hub import login\n",
    "\n",
    "from control_net_config import get_args_list, parse_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33myxy007\u001b[0m (\u001b[33mseeene\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.16.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Ene\\Desktop\\BU\\Spring24\\Project\\EC523_Project_G\\src\\wandb\\run-20240421_120707-mouj8pa7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/seeene/control_net_test/runs/mouj8pa7' target=\"_blank\">zesty-oath-18</a></strong> to <a href='https://wandb.ai/seeene/control_net_test' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/seeene/control_net_test' target=\"_blank\">https://wandb.ai/seeene/control_net_test</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/seeene/control_net_test/runs/mouj8pa7' target=\"_blank\">https://wandb.ai/seeene/control_net_test/runs/mouj8pa7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/seeene/control_net_test/runs/mouj8pa7?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x22f0d2d93a0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project=\"control_net_test\", entity=\"seeene\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Loading Metadata...\n",
      "series_name: usagi_s1_\n",
      "series_name: cute\n",
      "Finish Loading Metadata...\n"
     ]
    }
   ],
   "source": [
    "project_main_path = Path.cwd().parent\n",
    "added_path = os.path.abspath(project_main_path.__str__())\n",
    "\n",
    "PHASE3_SCENE_DESCRIPTION_FILE = \"DATASET/PROCESSING_RECORD_PHASE3_SCENE_DESCRIPTION_test.json\"\n",
    "added_path = os.path.abspath(project_main_path.__str__())\n",
    "if added_path not in os.sys.path:\n",
    "    os.sys.path.append(added_path)  \n",
    "dataset_path = os.path.abspath(project_main_path) # adjust the path to the dataset\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "DATSET_SHUFFLE = True\n",
    "MAX_NUM_FIGURE=1\n",
    "weight_dtype = torch.float32\n",
    "\n",
    "data_path_dict, anime_figure_scene_dataset = anime_data.get_dataset(PHASE3_SCENE_DESCRIPTION_FILE, dataset_path=dataset_path, MAX_NUM_FIGURE=MAX_NUM_FIGURE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "302"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(anime_figure_scene_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from control_net_train import get_args_list, parse_args, get_logger_and_accelerator, get_tokenizer, get_controlnet_unet, load_and_setting_models, get_dataloader, collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/21/2024 12:07:08 - INFO - __main__ - Distributed environment: NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: no\n",
      "\n",
      "{'upcast_attention', 'cross_attention_norm', 'class_embeddings_concat', 'conv_out_kernel', 'addition_embed_type', 'mid_block_type', 'conv_in_kernel', 'projection_class_embeddings_input_dim', 'resnet_time_scale_shift', 'attention_type', 'transformer_layers_per_block', 'num_attention_heads', 'mid_block_only_cross_attention', 'resnet_skip_time_act', 'timestep_post_act', 'encoder_hid_dim', 'time_cond_proj_dim', 'reverse_transformer_layers_per_block', 'time_embedding_act_fn', 'dropout', 'time_embedding_dim', 'time_embedding_type', 'addition_embed_type_num_heads', 'addition_time_embed_dim', 'resnet_out_scale_factor', 'encoder_hid_dim_type', 'class_embed_type'} was not found in config. Values will be initialized to default values.\n",
      "04/21/2024 12:07:08 - INFO - __main__ - Initializing controlnet weights from unet\n",
      "You are using a model of type clip_text_model to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
      "{'clip_sample_range', 'sample_max_value', 'variance_type', 'rescale_betas_zero_snr', 'dynamic_thresholding_ratio', 'thresholding', 'timestep_spacing'} was not found in config. Values will be initialized to default values.\n",
      "{'latents_std', 'latents_mean', 'scaling_factor', 'force_upcast'} was not found in config. Values will be initialized to default values.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MultiControlNetModel_SELF(\n",
       "  (nets): ModuleList(\n",
       "    (0-1): 2 x ControlNetModel_SELF(\n",
       "      (conv_in): Conv2d(4, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (time_proj): Timesteps()\n",
       "      (time_embedding): TimestepEmbedding(\n",
       "        (linear_1): Linear(in_features=320, out_features=1280, bias=True)\n",
       "        (act): SiLU()\n",
       "        (linear_2): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (controlnet_cond_embedding): ControlNetConditioningEmbedding(\n",
       "        (conv_in): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (blocks): ModuleList(\n",
       "          (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "          (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (3): Conv2d(32, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "          (4): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (5): Conv2d(96, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        )\n",
       "        (conv_out): Conv2d(256, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (down_blocks): ModuleList(\n",
       "        (0): CrossAttnDownBlock2D(\n",
       "          (attentions): ModuleList(\n",
       "            (0-1): 2 x Transformer2DModel(\n",
       "              (norm): GroupNorm(32, 320, eps=1e-06, affine=True)\n",
       "              (proj_in): Linear(in_features=320, out_features=320, bias=True)\n",
       "              (transformer_blocks): ModuleList(\n",
       "                (0): BasicTransformerBlock(\n",
       "                  (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "                  (attn1): Attention(\n",
       "                    (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "                    (to_k): Linear(in_features=320, out_features=320, bias=False)\n",
       "                    (to_v): Linear(in_features=320, out_features=320, bias=False)\n",
       "                    (to_out): ModuleList(\n",
       "                      (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "                      (1): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                  (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "                  (attn2): Attention(\n",
       "                    (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
       "                    (to_k): Linear(in_features=1024, out_features=320, bias=False)\n",
       "                    (to_v): Linear(in_features=1024, out_features=320, bias=False)\n",
       "                    (to_out): ModuleList(\n",
       "                      (0): Linear(in_features=320, out_features=320, bias=True)\n",
       "                      (1): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                  (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "                  (ff): FeedForward(\n",
       "                    (net): ModuleList(\n",
       "                      (0): GEGLU(\n",
       "                        (proj): Linear(in_features=320, out_features=2560, bias=True)\n",
       "                      )\n",
       "                      (1): Dropout(p=0.0, inplace=False)\n",
       "                      (2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (proj_out): Linear(in_features=320, out_features=320, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (resnets): ModuleList(\n",
       "            (0-1): 2 x ResnetBlock2D(\n",
       "              (norm1): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
       "              (conv1): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (time_emb_proj): Linear(in_features=1280, out_features=320, bias=True)\n",
       "              (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (nonlinearity): SiLU()\n",
       "            )\n",
       "          )\n",
       "          (downsamplers): ModuleList(\n",
       "            (0): Downsample2D(\n",
       "              (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): CrossAttnDownBlock2D(\n",
       "          (attentions): ModuleList(\n",
       "            (0-1): 2 x Transformer2DModel(\n",
       "              (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
       "              (proj_in): Linear(in_features=640, out_features=640, bias=True)\n",
       "              (transformer_blocks): ModuleList(\n",
       "                (0): BasicTransformerBlock(\n",
       "                  (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "                  (attn1): Attention(\n",
       "                    (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "                    (to_k): Linear(in_features=640, out_features=640, bias=False)\n",
       "                    (to_v): Linear(in_features=640, out_features=640, bias=False)\n",
       "                    (to_out): ModuleList(\n",
       "                      (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "                      (1): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                  (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "                  (attn2): Attention(\n",
       "                    (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
       "                    (to_k): Linear(in_features=1024, out_features=640, bias=False)\n",
       "                    (to_v): Linear(in_features=1024, out_features=640, bias=False)\n",
       "                    (to_out): ModuleList(\n",
       "                      (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "                      (1): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                  (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "                  (ff): FeedForward(\n",
       "                    (net): ModuleList(\n",
       "                      (0): GEGLU(\n",
       "                        (proj): Linear(in_features=640, out_features=5120, bias=True)\n",
       "                      )\n",
       "                      (1): Dropout(p=0.0, inplace=False)\n",
       "                      (2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (proj_out): Linear(in_features=640, out_features=640, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (resnets): ModuleList(\n",
       "            (0): ResnetBlock2D(\n",
       "              (norm1): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
       "              (conv1): Conv2d(320, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)\n",
       "              (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (nonlinearity): SiLU()\n",
       "              (conv_shortcut): Conv2d(320, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "            (1): ResnetBlock2D(\n",
       "              (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "              (conv1): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)\n",
       "              (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (nonlinearity): SiLU()\n",
       "            )\n",
       "          )\n",
       "          (downsamplers): ModuleList(\n",
       "            (0): Downsample2D(\n",
       "              (conv): Conv2d(640, 640, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): CrossAttnDownBlock2D(\n",
       "          (attentions): ModuleList(\n",
       "            (0-1): 2 x Transformer2DModel(\n",
       "              (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
       "              (proj_in): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "              (transformer_blocks): ModuleList(\n",
       "                (0): BasicTransformerBlock(\n",
       "                  (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "                  (attn1): Attention(\n",
       "                    (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                    (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                    (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                    (to_out): ModuleList(\n",
       "                      (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                      (1): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                  (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "                  (attn2): Attention(\n",
       "                    (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                    (to_k): Linear(in_features=1024, out_features=1280, bias=False)\n",
       "                    (to_v): Linear(in_features=1024, out_features=1280, bias=False)\n",
       "                    (to_out): ModuleList(\n",
       "                      (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                      (1): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                  (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "                  (ff): FeedForward(\n",
       "                    (net): ModuleList(\n",
       "                      (0): GEGLU(\n",
       "                        (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
       "                      )\n",
       "                      (1): Dropout(p=0.0, inplace=False)\n",
       "                      (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (proj_out): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (resnets): ModuleList(\n",
       "            (0): ResnetBlock2D(\n",
       "              (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "              (conv1): Conv2d(640, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "              (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (nonlinearity): SiLU()\n",
       "              (conv_shortcut): Conv2d(640, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "            (1): ResnetBlock2D(\n",
       "              (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "              (conv1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "              (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (nonlinearity): SiLU()\n",
       "            )\n",
       "          )\n",
       "          (downsamplers): ModuleList(\n",
       "            (0): Downsample2D(\n",
       "              (conv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (3): DownBlock2D(\n",
       "          (resnets): ModuleList(\n",
       "            (0-1): 2 x ResnetBlock2D(\n",
       "              (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "              (conv1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "              (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (nonlinearity): SiLU()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (controlnet_down_blocks): ModuleList(\n",
       "        (0-3): 4 x Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (4-6): 3 x Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (7-11): 5 x Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (controlnet_mid_block): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (mid_block): UNetMidBlock2DCrossAttn(\n",
       "        (attentions): ModuleList(\n",
       "          (0): Transformer2DModel(\n",
       "            (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
       "            (proj_in): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (transformer_blocks): ModuleList(\n",
       "              (0): BasicTransformerBlock(\n",
       "                (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn1): Attention(\n",
       "                  (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                  (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                  (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                  (to_out): ModuleList(\n",
       "                    (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn2): Attention(\n",
       "                  (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                  (to_k): Linear(in_features=1024, out_features=1280, bias=False)\n",
       "                  (to_v): Linear(in_features=1024, out_features=1280, bias=False)\n",
       "                  (to_out): ModuleList(\n",
       "                    (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "                (ff): FeedForward(\n",
       "                  (net): ModuleList(\n",
       "                    (0): GEGLU(\n",
       "                      (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
       "                    )\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                    (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (proj_out): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (resnets): ModuleList(\n",
       "          (0-1): 2 x ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "            (conv1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if os.path.exists('/projectnb/dl523/students/ychuang2/'):\n",
    "        HUGGING_FACE_CACHE_DIR = \"/projectnb/dl523/students/ychuang2/.cache/huggingface/diffusers\"\n",
    "else:\n",
    "    HUGGING_FACE_CACHE_DIR = \"./cache\"\n",
    "\n",
    "logger = get_logger(__name__)\n",
    "\n",
    "checkpointing_steps = 1600 // (BATCH_SIZE // 4) // 2\n",
    "validation_steps = checkpointing_steps \n",
    "num_train_epochs = 50\n",
    "checkpoint = \"latest\"\n",
    "\n",
    "args_list = get_args_list(BATCH_SIZE, num_train_epochs, checkpointing_steps, validation_steps)\n",
    "args = parse_args(BATCH_SIZE, input_args = args_list)\n",
    "\n",
    "accelerator, logger, repo_id = get_logger_and_accelerator(args, logger)\n",
    "\n",
    "tokenizer = get_tokenizer(args, accelerator)\n",
    "controlnet, unet = get_controlnet_unet(args, accelerator, HUGGING_FACE_CACHE_DIR, logger)\n",
    "\n",
    "if checkpoint == \"latest\":\n",
    "    controlnet_1 = ControlNetModel.from_pretrained(\n",
    "                f\"model_out_1\", torch_dtype=weight_dtype,\n",
    "                cache_dir = HUGGING_FACE_CACHE_DIR,\n",
    "                )\n",
    "    controlnet_2 = controlnet_1\n",
    "else:\n",
    "    controlnet_1 = ControlNetModel.from_pretrained(\n",
    "                f\"model_out/{checkpoint}/controlnet\", torch_dtype=weight_dtype,\n",
    "                cache_dir = HUGGING_FACE_CACHE_DIR,\n",
    "                )\n",
    "\n",
    "    controlnet_2 = ControlNetModel.from_pretrained(\n",
    "                f\"model_out/{checkpoint}/controlnet_1\", torch_dtype=weight_dtype,\n",
    "                cache_dir = HUGGING_FACE_CACHE_DIR,\n",
    "                )\n",
    "\n",
    "# to multi-controlnet, copy the same onw twice , 1 -> 2x\n",
    "controlnet = MultiControlNetModel_SELF([controlnet_1, controlnet_2])\n",
    "\n",
    "noise_scheduler, text_encoder, vae = load_and_setting_models(args, accelerator, HUGGING_FACE_CACHE_DIR, logger)\n",
    "\n",
    "vae.requires_grad_(False)\n",
    "unet.requires_grad_(False)\n",
    "text_encoder.requires_grad_(False)\n",
    "controlnet.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def collate_fn(examples):\n",
    "    # pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
    "    pixel_values = torch.stack([torch.tensor(example[\"scene_img\"]) for example in examples])\n",
    "    pixel_values = pixel_values.to(memory_format=torch.contiguous_format).float()\n",
    "    \n",
    "    conditioning_pixel_values = torch.stack([torch.tensor(example[\"inpainting_img\"]) for example in examples])\n",
    "    conditioning_pixel_values = conditioning_pixel_values.to(memory_format=torch.contiguous_format).float()\n",
    "\n",
    "\n",
    "    figures = []\n",
    "    for example in examples:\n",
    "        img = example[\"figure_img_list\"]\n",
    "        # print(img.shape)    \n",
    "        if img.ndim == 4:\n",
    "            figures.append(torch.tensor(img))\n",
    "        elif img.ndim == 5:\n",
    "            figures.append(torch.tensor(img[:,0,:,:,:]))\n",
    "        else:\n",
    "            raise ValueError(\"figure_img_list should have 4 or 5 dimensions\")        \n",
    "    # conditioning_pixel_values_02 = torch.stack([torch.tensor(example[\"figure_img_list\"][:,0,:,:,:]) for example in examples])\n",
    "    conditioning_pixel_values_02 = torch.cat(figures, dim=0)\n",
    "    conditioning_pixel_values_02 = conditioning_pixel_values_02.to(memory_format=torch.contiguous_format).float()\n",
    "\n",
    "\n",
    "    captions = [example[\"description\"] for example in examples]\n",
    "    inputs = tokenizer(\n",
    "        captions, max_length=tokenizer.model_max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n",
    "    )\n",
    "    # input_ids = torch.stack([example[\"input_ids\"] for example in examples])\n",
    "    input_ids = inputs.input_ids\n",
    "\n",
    "    return {\n",
    "        \"pixel_values\": pixel_values,\n",
    "        \"conditioning_pixel_values\": conditioning_pixel_values,\n",
    "        \"conditioning_pixel_values_02\": conditioning_pixel_values_02,\n",
    "        \"input_ids\": input_ids,\n",
    "        \"captions\": captions\n",
    "    }\n",
    "\n",
    "def get_evaluation_dataloader(args, tokenizer, accelerator, dataset, split_ratio=0):\n",
    "    # Calculate the length of the dataset and the split point\n",
    "    len_dataset = len(dataset)\n",
    "    len_eval = int((1 - split_ratio) * len_dataset)\n",
    "\n",
    "    # Create a subset for the evaluation portion of the dataset\n",
    "    dataset_eval = torch.utils.data.Subset(dataset, range(len(dataset) - len_eval, len_dataset))\n",
    "    \n",
    "    # Create the dataloader for the evaluation dataset\n",
    "    eval_dataloader = torch.utils.data.DataLoader(\n",
    "        dataset_eval,\n",
    "        shuffle=False,  # Typically, shuffling is not needed for evaluation\n",
    "        collate_fn=collate_fn,  # Ensure your collate function is defined elsewhere in your script\n",
    "        batch_size=args.train_batch_size,  # Consider using a separate batch size for eval if needed\n",
    "        num_workers=args.dataloader_num_workers,\n",
    "    )\n",
    "\n",
    "    return eval_dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = get_evaluation_dataloader(args, tokenizer, accelerator, anime_figure_scene_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\control\\lib\\site-packages\\diffusers\\models\\attention_processor.py:1279: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  hidden_states = F.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 1\n",
      "step: 2\n",
      "step: 3\n",
      "step: 4\n",
      "step: 5\n",
      "step: 6\n",
      "step: 7\n",
      "step: 8\n",
      "step: 9\n",
      "Average loss: 0.08332978785037995\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x22f0b154f40>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ensure all models are in evaluation mode\n",
    "vae.eval()\n",
    "text_encoder.eval()\n",
    "unet.eval()\n",
    "controlnet.eval()\n",
    "\n",
    "# Disable gradient computation for evaluation to save memory and computations\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "# all to device\n",
    "vae.to(accelerator.device)\n",
    "text_encoder.to(accelerator.device)\n",
    "unet.to(accelerator.device)\n",
    "controlnet.to(accelerator.device)\n",
    "\n",
    "\n",
    "# Initialize variables for metrics\n",
    "total_loss = 0\n",
    "num_batches = 0\n",
    "weight_dtype = torch.float32\n",
    "\n",
    "# Process each batch in the dataloader\n",
    "for step, batch in enumerate(test_dataloader):\n",
    "    print(f\"step: {step}\")\n",
    "    # Convert images to latent space\n",
    "    \n",
    "    latents = vae.encode(batch[\"pixel_values\"].to(accelerator.device, dtype=weight_dtype)).latent_dist.sample()\n",
    "    latents = latents * vae.config.scaling_factor\n",
    "\n",
    "    # Sample noise that we'll add to the latents\n",
    "    noise = torch.randn_like(latents)\n",
    "    bsz = latents.shape[0]\n",
    "    # Sample a random timestep for each image\n",
    "    timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (bsz,), device=latents.device)\n",
    "    timesteps = timesteps.long()\n",
    "\n",
    "    # Add noise to the latents according to the noise magnitude at each timestep (forward diffusion process)\n",
    "    noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
    "\n",
    "    # Get the text embeddings for conditioning\n",
    "    encoder_hidden_states = text_encoder(batch[\"input_ids\"].to(accelerator.device), return_dict=False)[0]\n",
    "\n",
    "    controlnet_image_01 = batch[\"conditioning_pixel_values\"].to(accelerator.device, dtype=weight_dtype)\n",
    "    controlnet_image_02 = batch[\"conditioning_pixel_values_02\"].to(accelerator.device, dtype=weight_dtype)\n",
    "\n",
    "    # Inference using the controlnet model\n",
    "    assert isinstance(controlnet, MultiControlNetModel_SELF)\n",
    "    down_block_res_samples, mid_block_res_sample = controlnet(\n",
    "        noisy_latents,\n",
    "        timesteps,\n",
    "        encoder_hidden_states=encoder_hidden_states,\n",
    "        controlnet_cond=[controlnet_image_01, controlnet_image_02],\n",
    "        return_dict=False,\n",
    "        conditioning_scale=[1.0]*2,\n",
    "    )\n",
    "\n",
    "    # Generate predictions from the UNet model\n",
    "    model_pred = unet(\n",
    "        noisy_latents,\n",
    "        timesteps,\n",
    "        encoder_hidden_states=encoder_hidden_states,\n",
    "        down_block_additional_residuals=[sample.to(dtype=weight_dtype) for sample in down_block_res_samples],\n",
    "        mid_block_additional_residual=mid_block_res_sample.to(dtype=weight_dtype),\n",
    "        return_dict=False,\n",
    "    )[0]\n",
    "\n",
    "        \n",
    "    \n",
    "    # Calculate the target for loss based on the prediction type\n",
    "    if noise_scheduler.config.prediction_type == \"epsilon\":\n",
    "        target = noise\n",
    "    elif noise_scheduler.config.prediction_type == \"v_prediction\":\n",
    "        target = noise_scheduler.get_velocity(latents, noise, timesteps)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown prediction type {noise_scheduler.config.prediction_type}\")\n",
    "\n",
    "    # Compute loss\n",
    "    loss = F.mse_loss(model_pred.float(), target.float(), reduction=\"mean\")\n",
    "    total_loss += loss.item()\n",
    "    num_batches += 1\n",
    "\n",
    "# Calculate average loss\n",
    "average_loss = total_loss / num_batches if num_batches > 0 else 0\n",
    "print(f\"Average loss: {average_loss}\")\n",
    "\n",
    "# Re-enable gradient computation\n",
    "torch.set_grad_enabled(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if args.resume_from_checkpoint:\n",
    "#         print(f\"resume_from_checkpoint\")\n",
    "#         if args.resume_from_checkpoint != \"latest\":\n",
    "#             path = os.path.basename(args.resume_from_checkpoint)\n",
    "#         else:\n",
    "#             # Get the most recent checkpoint\n",
    "#             dirs = os.listdir(args.output_dir)\n",
    "#             dirs = [d for d in dirs if d.startswith(\"checkpoint\")]\n",
    "#             dirs = sorted(dirs, key=lambda x: int(x.split(\"-\")[1]))\n",
    "#             path = dirs[-1] if len(dirs) > 0 else None\n",
    "\n",
    "#         if path is None:\n",
    "#             accelerator.print(\n",
    "#                 f\"Checkpoint '{args.resume_from_checkpoint}' does not exist. Starting a new training run.\"\n",
    "#             )\n",
    "#             args.resume_from_checkpoint = None\n",
    "#             initial_global_step = 0\n",
    "#         else:\n",
    "#             path = \"checkpoint-7714\"\n",
    "#             accelerator.print(f\"Resuming from checkpoint {path}\")\n",
    "#             accelerator.load_state(os.path.join(args.output_dir, path))\n",
    "#             global_step = int(path.split(\"-\")[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# controlnet = accelerator.unwrap_model(controlnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import functional as TF\n",
    "# login wandb\n",
    "def log_images_to_wandb(image_logs):\n",
    "    formatted_images = []\n",
    "    index = 0\n",
    "    for log in image_logs:\n",
    "        images = log[\"gen_images\"]\n",
    "        validation_prompt = log[\"validation_prompt\"]\n",
    "        validation_image = log[\"validation_image\"]\n",
    "        cond_01_image = log[\"cond_01\"]\n",
    "        cond_02_image = log[\"cond_02\"]\n",
    "        \n",
    "        # Assuming images are in the range [0, 1], scale to [0, 255] and convert to numpy arrays\n",
    "        validation_image_np = ((validation_image.detach().cpu().numpy() + 0.5) * 255).astype(np.uint8)\n",
    "        cond_01_image_np = ((cond_01_image.detach().cpu().numpy() + 0.5) * 255).astype(np.uint8)\n",
    "        cond_02_image_np = ((cond_02_image.detach().cpu().numpy() + 0.5) * 255).astype(np.uint8)\n",
    "        \n",
    "        formatted_images.append(wandb.Image(validation_image_np, caption=\"Controlnet conditioning\"))\n",
    "        formatted_images.append(wandb.Image(cond_01_image_np, caption=\"Controlnet conditioning 01\"))\n",
    "        formatted_images.append(wandb.Image(cond_02_image_np, caption=\"Controlnet conditioning 02\"))   \n",
    "\n",
    "        sub_index = 0\n",
    "        for image in images:\n",
    "            image.save(f\"output_image/generated/generated_progress_{index}_{sub_index}.png\")\n",
    "            image = wandb.Image(image, caption=validation_prompt)\n",
    "            formatted_images.append(image)\n",
    "            sub_index += 1\n",
    "                \n",
    "        # save to local file\n",
    "        # save validation_image_np\n",
    "        TF.to_pil_image(validation_image_np).save(f\"output_image/ground_truth/ground_truth_progress_{index}.png\")\n",
    "        TF.to_pil_image(cond_01_image_np).save(f\"output_image/inpainting/inpainting_progress_{index}.png\")\n",
    "        TF.to_pil_image(cond_02_image_np).save(f\"output_image/character/character_progress_{index}.png\")\n",
    "        with open(f'output_image/text/text_progress_{index}.txt', 'w') as f:\n",
    "            f.write(validation_prompt)\n",
    "        \n",
    "        index += 1\n",
    "            \n",
    "    print(len(formatted_images))\n",
    "    wandb.log({\"images\": formatted_images})   \n",
    "    wandb.log({\"checkpoint\": checkpoint})\n",
    "    wandb.log({\"test lost\": average_loss})   \n",
    "    \n",
    "   \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{'image_encoder', 'controlnet'} was not found in config. Values will be initialized to default values.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15cf6c02f8d548398f5febd25fd94e41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{'timestep_spacing'} was not found in config. Values will be initialized to default values.\n",
      "Loaded scheduler as PNDMScheduler from `scheduler` subfolder of stabilityai/stable-diffusion-2-1-base.\n",
      "Loaded feature_extractor as CLIPImageProcessor from `feature_extractor` subfolder of stabilityai/stable-diffusion-2-1-base.\n",
      "{'use_karras_sigmas', 'predict_x0', 'lower_order_final', 'disable_corrector', 'sample_max_value', 'solver_order', 'dynamic_thresholding_ratio', 'solver_p', 'solver_type', 'thresholding', 'timestep_spacing'} was not found in config. Values will be initialized to default values.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c5a5fcdc301442f9b87e49d54551908",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/10 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (89 > 77). Running this sequence through the model will result in indexing errors\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['of the scene, with the woman and girl occupying it']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['consists of a wall and a door, which are both located on the right side of the image']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['woman, and they seem to be interacting with each other']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['comfortable and inviting setting, possibly a cafe or a cozy living space']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['addition to the tea set and utensils, there are two people in the image']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['side of the table']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['child is positioned to the right']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['ious composition']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['and right sides of the frame']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the background. the person holding the red cloth seems to be presenting or showing the cloth to the other person, who is observing the situation']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['element to the scene, creating a sense of depth and contrast']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['a backdrop, creating a sense of depth and context']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['scattered around the table, while the bowls are positioned closer to the center']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['of greenery to the space']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['and bow, contribute to her overall appearance']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['her create a visually appealing and vibrant atmosphere']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['image']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the figures and others further away. the cups are also dispersed throughout the bar, with some near the figures and others in the background']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['a street or a park, where people are going about their daily activities']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['entering or exiting the room']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['object. the bed serves as a backdrop, indicating that the woman is in a personal environment']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['her presence dominates the image']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['on the right side']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['and relaxation']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the presence of the door and the window indicates that the room is likely a part of a house or an apartment']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [', one near the top left corner and the other near the top right corner']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['creates a dramatic and atmospheric backdrop for the figures']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['sense of depth and perspective']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['is located towards the right side of the image']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['located near the woman and the other further away']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the cup is held by the girl']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['cold weather, indicating that she is prepared for the snowy conditions']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['side']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['rest in']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [', and the woman is positioned in the center of the room']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['of the room. the wall is visible on the right side of the room']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the image and the other person located towards the right side. a book can be seen on the bench, close to the girl']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['scene, and her presence adds a lively atmosphere to the bar']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [', which adds an element of decoration to the scene']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['scattered around the room']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the presence of the cat figurine suggests that the person might have an interest in cats or a fondness for the figurine']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['on the bed. the chair is located near the bed, and the boy is lying on the bed next to the girl']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['and striking appearance']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['other on the right side']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['positioned on the right side. the building is situated in the background, creating a sense of depth and perspective in the scene']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the left side of the house, while the window is situated on the right side']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['shirt']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [', as there are trees in the background']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['other objects are placed around her. the tv is located on the left side of the room, while the chair is on the right side']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['nearby']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the right side']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['trees, which add a touch of greenery to the scene']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['or have an interest in literature']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['presence of a building in the background indicates that she is in an urban or residential area']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['nearby']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['engaging in an activity that requires sound']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['listening to something while being outdoors']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [\"different positions, indicating that they are part of the counter's display or arrangement\"]\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['main subject of the image']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['composition']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['image, and a book is visible on the left side as well']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the middle of the space, and a dining table can be seen in the background']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['pointing at it suggests that they are drawing attention to it or highlighting its significance. the surrounding area consists of a wall and a window, which are located on the left side of the scene']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the middle, and the bookshelf is on the right side']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the sign is that she is standing in front of it, possibly indicating her involvement with the sign or the content it displays']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['and others further away']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['right side']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the woman']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['or exiting the room, or simply observing their surroundings']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['only figure in the image, and it is the only figure that is in motion, extending outward']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['bottles placed around the kitchen, possibly containing beverages or condiments']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['area is empty, with no other figures or objects visible in the image']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['backpack located near the man, possibly indicating that he is in a public space or on the move']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['touch of color to the scene']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [', and he is the main focus of the scene']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['surprise or excitement']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['presence of the books suggests that the person might be in a study or a reading area']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['waiting for someone. the presence of the hooded jacket and the nighttime setting indicate that the person is likely trying to stay warm or protect themselves from the cold weather']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['of depth and adds a natural element to the image']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['surrounding environment, including the fence and trees, adds context to the scene and provides a sense of location']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['table visible in the background']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['with low lighting']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['of depth and tranquility']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['in the beautiful setting']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['the image. the building in the background serves as a backdrop, providing context to the scene']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['while the other two people are walking in the opposite direction. the scene is set in a city environment, with a building visible in the background']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['of the path, while the other figures are positioned around them']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [\"and the temple, possibly indicating a visit or a connection to the temple's spiritual or cultural significance\"]\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['a city or town']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['happiness to the image']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['interest to the image']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['on the right side of the image']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['in the background, one on the left side and the other on the right side of the hallway']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [', drawing attention to the woman']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import contextlib\n",
    "\n",
    "def run_evaluation(eval_dataloader, vae, text_encoder, unet, controlnet, args, accelerator, weight_dtype):\n",
    "    # Set all models to evaluation mode\n",
    "    controlnet.eval()\n",
    "    vae.eval()\n",
    "    text_encoder.eval()\n",
    "    unet.eval()\n",
    "\n",
    "    # Disable gradients to speed up the process and reduce memory usage\n",
    "    torch.set_grad_enabled(False)\n",
    "\n",
    "    # Set up the evaluation pipeline\n",
    "    # controlnet = accelerator.unwrap_model(controlnet)\n",
    "    # controlnet = ControlNetModel.from_pretrained(\n",
    "    #         \"model_out_1\", torch_dtype=weight_dtype,\n",
    "    #         cache_dir = HUGGING_FACE_CACHE_DIR,\n",
    "    #         )\n",
    "\n",
    "    pipeline = StableDiffusionControlNetPipeline.from_pretrained(\n",
    "        args.pretrained_model_name_or_path,\n",
    "        vae=vae,\n",
    "        text_encoder=text_encoder,\n",
    "        tokenizer=tokenizer,\n",
    "        unet=unet,\n",
    "        controlnet=controlnet,\n",
    "        # controlnet=[controlnet, controlnet],\n",
    "        safety_checker=None,\n",
    "        revision=args.revision,\n",
    "        variant=args.variant,\n",
    "        torch_dtype=weight_dtype,\n",
    "        cache_dir = HUGGING_FACE_CACHE_DIR,\n",
    "    )\n",
    "    pipeline.scheduler = UniPCMultistepScheduler.from_config(pipeline.scheduler.config)\n",
    "    pipeline = pipeline.to(accelerator.device)\n",
    "    pipeline.set_progress_bar_config(disable=True)\n",
    "\n",
    "    \n",
    "    if args.enable_xformers_memory_efficient_attention:\n",
    "        pipeline.enable_xformers_memory_efficient_attention()\n",
    "        \n",
    "    if args.seed is None:\n",
    "        generator = None\n",
    "    else:\n",
    "        generator = torch.Generator(device=accelerator.device).manual_seed(args.seed)\n",
    "\n",
    "    # Prepare for inference context\n",
    "    global images\n",
    "    image_logs = []\n",
    "    inference_ctx = torch.autocast(\"cuda\")\n",
    "\n",
    "    i_image = 0\n",
    "    for batch in tqdm(eval_dataloader, desc=\"Validation\", unit=\"batch\"):\n",
    "        B = batch[\"pixel_values\"].shape[0]\n",
    "        for i in range(B):\n",
    "            # print(batch[\"captions\"][i])\n",
    "            # print(batch[\"conditioning_pixel_values\"][i].shape, batch[\"conditioning_pixel_values_02\"][i].shape)\n",
    "    \n",
    "            i_image += 1\n",
    "            with inference_ctx:\n",
    "                images = pipeline(\n",
    "                    batch[\"captions\"][i],\n",
    "                    # batch[\"scene_img\"][i],\n",
    "                    image=[batch[\"conditioning_pixel_values\"][i][None, :, :, :], batch[\"conditioning_pixel_values_02\"][i][None, :, :, :]], # inpainting_img, figure_img\n",
    "                    # num_inference_steps=20,\n",
    "                    num_inference_steps=50,\n",
    "                    # num_inference_steps=1000,\n",
    "                    generator=generator,\n",
    "                    num_images_per_prompt=3,\n",
    "                ).images\n",
    "\n",
    "            import torchvision\n",
    "            img_grid_gen = torchvision.utils.make_grid([torch.tensor(np.array(image)).permute(2,0,1)for image in images], nrow=5)\n",
    "            img_grid_gen = img_grid_gen.permute(1,2,0)\n",
    " \n",
    "            image_logs.append(\n",
    "                {\"validation_image\": batch[\"pixel_values\"][i].detach().cpu().data.permute(1, 2, 0), \n",
    "                 \"cond_01\": batch[\"conditioning_pixel_values\"][i].detach().cpu().data.permute(1, 2, 0),\n",
    "                 \"cond_02\": batch[\"conditioning_pixel_values_02\"][i].detach().cpu().data.permute(1, 2, 0),\n",
    "                 \"gen_images\": images, \n",
    "                 \"validation_prompt\": batch[\"captions\"][i]}\n",
    "            )\n",
    "\n",
    "        \n",
    "    return image_logs\n",
    "\n",
    "# Example call to the function, assuming necessary objects and arguments are defined:\n",
    "image_logs = run_evaluation(test_dataloader, vae, text_encoder, unet, controlnet, args, accelerator, torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "302"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(image_logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/21/2024 12:28:57 - WARNING - root - Only 108 Image will be uploaded.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1812\n"
     ]
    }
   ],
   "source": [
    "log_images_to_wandb(image_logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "control",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
